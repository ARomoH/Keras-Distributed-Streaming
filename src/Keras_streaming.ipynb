{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS MODEL IN STREAM CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import Row, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout, Merge, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras.layers import advanced_activations\n",
    "from keras import initializers\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local[8]\")\n",
    "    .config(\"spark.driver.cores\", 8)\n",
    "    .appName(\"KerasStream\")\n",
    "    .getOrCreate() )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kafkaParams = {\"metadata.broker.list\": \"localhost:9092\"}\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = open('./models/CNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "data_model = sc.broadcast(loaded_model_json)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred(data):\n",
    "    tam_fijo = 15\n",
    "    embedding_vecor_length = 300\n",
    "    # n_filters = 3\n",
    "\n",
    "    loaded_model = model_from_json(data_model.value)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"./models/CNN_model.h5\")\n",
    "    \n",
    "    numpVect = data.split()\n",
    "    cont_num = 0\n",
    "    cont_words = 0\n",
    "    X_test = np.zeros((1, tam_fijo, embedding_vecor_length))\n",
    "        \n",
    "    for num in numpVect:\n",
    "        if cont_num == 300:\n",
    "            cont_words += 1\n",
    "            cont_num = 0\n",
    "            # print(str(cont_tweet) + ' ' + str(cont_words) + ' ' + str(cont_num))\n",
    "        X_test[0][cont_words][cont_num]= num\n",
    "        cont_num += 1\n",
    "    cont_words = 0\n",
    "    cont_num = 0\n",
    "    \n",
    "    prediction = loaded_model.predict(X_test)\n",
    "    sent = np.argmax(prediction)\n",
    "    \n",
    "    if sent == 1:\n",
    "        return 'POSITIVE'\n",
    "    else:\n",
    "        return 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = directKafkaStream.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    # Get the singleton instance of SparkSession\n",
    "    spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "    rowRdd4 = rdd.map(lambda w: Row(sen=get_pred(w)))\n",
    "    df4 = spark.createDataFrame(rowRdd4)\n",
    "        \n",
    "    df4.show()\n",
    "\n",
    "\n",
    "\n",
    "instances.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute next line in terminal:\n",
    "** python kafka_user_producer.py test **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2017-09-19 10:21:45 =========\n",
      "========= 2017-09-19 10:21:48 =========\n",
      "========= 2017-09-19 10:21:51 =========\n",
      "+--------+\n",
      "|     sen|\n",
      "+--------+\n",
      "|POSITIVE|\n",
      "+--------+\n",
      "\n",
      "========= 2017-09-19 10:21:54 =========\n",
      "========= 2017-09-19 10:21:57 =========\n",
      "+--------+\n",
      "|     sen|\n",
      "+--------+\n",
      "|NEGATIVE|\n",
      "+--------+\n",
      "\n",
      "========= 2017-09-19 10:22:00 =========\n"
     ]
    }
   ],
   "source": [
    "# First, execute only one of the example cells above; then, start the StreamingContext as follows\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once you are done, stop the StreamingContext; for the next run you must recreate it\n",
    "# again since the beginning (cell #2)\n",
    "ssc.stop(False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
